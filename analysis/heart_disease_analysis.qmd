---
title: Heart Disease Risk Prediction and Early-Stage Heart Disease Detection
author:
  - Sarisha Das
  - Mantram Sharma
  - Omowunmi Obadero
  - Shrabanti Bala Joya
date: "06 December 2025"
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.17.3
  kernelspec:
    display_name: Python [conda env:base] *
    language: python
    name: conda-base-py
format:
  html:
    toc: true
    number-sections: true
    toc-depth: 3
  pdf:
    toc: true
    number-sections: true
    toc-depth: 3
execute:
  echo: false
  warning: false
  message: false
bibliography: references.bib
link-citations: true
---


# 1. Summary

We wish to create a simple machine learning classification model which can help us predict high risk individuals for heart disease. We try three methods: Decision Tree Classifier, Logistic Regression and Support Vector Machine with Radial Basis Function (RBF) Kernel to use 14 common features related to heart disease to make the predictions. Here we aimed to find the best model that predicts whether an individual is at risk of developing heart disease based on their clinical features, enabling early identification and prevention measures. 

We have selected F2 score as our primary performance metric since our primary goal is to minimize False Negatives - cases where patients at risk of heart disease are incorrectly identified as healthy. The final classifier SVM RBF performed reasonably well on the unseen test dataset, achieving an F2 score (β = 2) of **replace with inline code 0.9646** and an overall accuracy of **replace with inline code 0.98**. Out of the 200 test data cases, it correctly predicted 196 and misclassified 4, which are all False Positives - predicting that a patient is at risk of developing heart disease when they are in fact healthy. False positives are not as dangerous as False Negatives. Although they could theoretically cause the patient to undergo unnecessary treatment if the model is used as a decision tool, we expect there to be additional decision layers which can mitigate this. As such, we believe this model serves as a valuable decision-support tool, assisting medical professionals in identifying high-risk individuals for closer monitoring and timely intervention.

# 2. Introduction

According to the American Heart Association @aha_ischemic,Heart disease, or Coronary Artery Disease is a condition in which narrowed coronary arteries reduce blood flow to the heart. This can lead to heart attack - where the heart can stop working, and in many cases leaves a very narrow window of time for responsive action. Data from the World Health Organization @who_leading_causes shows that, in India, a country of approximately 1.4 billion people, heart disease has consistently been the leading cause of death over the past decade (2010–2020) for both genders.

Cardiovascular diseases or CVDs account for about 31% of all deaths, according to the latest Sample Registration System report @bstd_cardio_2025. India’s age-standardized CVD death rate is estimated at 272 per 100,000, significantly higher than the global average of approximately 235 per 100,000 @Prabhakaran2016. 

If high-risk individuals can be identified before clinical events such as heart attacks, early interventions like the few recommended by the National Heart, Lung, and Blood Institute @nhlbi_chd_treatment can reduce mortality rate. The list includes simple lifestyle changes, to medicines like statins which can reduce plaque buildup, to medical procedures in necessary cases.

Since traditional diagnosis often depends on physician expertise, subjective assessment, and resource-intensive tests, a data-driven predictive model could therefore help identify patients who are more prone to such events, especially in resource-limited settings @GUPTA2018S419.

Thus, even minor improvements or supplementary methods in early detection could make a meaningful difference in population health. In this project, we attempt to use measurable, structured features to identify high-risk cases, enabling more careful monitoring and earlier preventive measures.

# 3. Methods

## 3.1 Data

The dataset contains 1000 unique examples and 14 features containing information on the individuals cholesterol, blood pressure and fasting blood sugar. We are using 13 features and dropping the patient ID feature for our analysis. The target variable indicates whether a patient has 'heart disease' or 'no heart disease'. The dataset has **no missing values**.

This dataset has been obtained from Doppala and Bhattacharyya @Doppala2021. It was collected at a multispecialty hospital in India. The original source provides detailed descriptions for all variables, along with summary statistics for the numerical features. The details can be found [here](https://github.com/sjbalagit/Heart-Disease-Predictor---Group16/blob/main/data/raw/Cardiovascular_Disease_Dataset/Cardiovascular_Disease_Dataset_Description.pdf).

## 3.2 Analysis


The analysis was performed using the Python programming language @Python and the following packages: pandas @pandas, altair @altair, and scikit-learn @scikit-learn. The dataset was split into training (80%) and testing (20%) subsets using a fixed random state (123) to ensure reproducibility. Stratified sampling was applied to maintain the proportion of heart disease cases across both sets. EDA was performed exclusively on the training data to prevent information leakage.


### 3.2.1 Data Preparation

To ensure data integrity, the dataset was successfully read and validated against a predefined pandera @niels_bantilan-proc-scipy-2020 schema, confirming that the file format, column names, and data types were consistent with analysis requirements. The validation process verified the absence of duplicate records or empty observations. Specific columns like slope and serum_cholesterol triggered warnings for zero-values and potential outliers. These anomalous values were addressed through imputation in the source dataset. We scaled the features to effectively capture the outliers rather than removing them to preserve data volume. Finally, we renamed the target labels to "Heart Disease" and "No Heart Disease" from binary encoding of 1 and 0 and inspected feature distributions and correlations. The results confirmed that no anomalous relationships exist that would impact modeling.


### 3.2.2 Exploratory Data Analysis

This section provides a detailed exploration of the dataset, focusing on the distribution of features, relationships with the target variable and insights that support later preprocessing and modeling.

**Target Distribution:** The dataset is relatively balanced (@fig-targ-dist), with 398 cases of heart disease and 302 cases without. This slight imbalance is not substantial enough to negatively impact modeling.

![Distribution of Target Variable in Input Data.](){#fig-targ-dist width=60%}

**Feature Distributions:** As seen in @fig-num-hist, none of the continuous features follow a perfect normal distribution. Notably, serum cholesterol and oldpeak are right-skewed, while maximum heart rate is left-skewed.

![Histograms of Numerical Features.](){#fig-num-hist width=60%}

**Numerical Features:** Patients with heart disease usually are older and generally exhibit higher resting blood pressure, serum cholesterol, and 'oldpeak' values compared to healthy individuals. On the other hand, they achieve notably lower maximum heart rates, suggesting reduced cardiac capacity under stress.

![Boxplots of Numeric Features.](){#fig-num-box width=60%}

**Categorical Features:** Higher heart disease prevalence is observed (@fig-cat-dist) in males and patients with exercise-induced angina, ST-T wave abnormalities, or a higher count of major vessels. Additionally, atypical and non-anginal chest pain types showed a stronger association with heart disease than typical angina in this dataset.


![Distribution of Categorical Features.](){#fig-cat-dist width=60%}

**Correlations:** The correlation heatmap (@fig-corr-map) reveals that slope, type of chest pain, and resting blood pressure have the strongest positive relationships with the target. On the other hand, age and gender showed surprisingly low linear correlation with the target in this specific dataset.

![Correlation Heatmap for All Features.](../results/correlation_heatmap.png){#fig-corr-map width=70%}


## 3.3 Scoring Metric**

We selected F2 score as our primary metric. In medical diagnostics, minimizing False Negatives is critical as failing to identify a patient at risk of developing heart disease carries more significance than incorrectly identifying a healthly patient as at risk - False Positive. The F-beta score weighs recall higher than precision, ensuring the model prioritizes capturing as many positive cases as possible.


$$
F_{\beta} = (1+\beta^2) . \frac{precision \times recall}{\beta^2 precision + recall}
$$

Setting $\beta = 2$ prioritizes recall twice as much as precision.

## 3.4 Model Tuning

We evaluated three candidate models: Logistic Regression, Support Vector Machine (SVM) with RBF kernel, and Decision Tree Classifier along with Dummy Classifier as the baseline. Initial performance was assessed using 5-fold cross-validation with default hyperparameters and random state 123 for an initial assesment of the models on the training set. The mean and standard deviation of these validation scores are summarized in @tbl-cv-default.

```{python}
#| label: tbl-cv-default
#| tbl-cap: 5-fold CV scores for different models with default parameters

```

Following the baseline evaluation, we employed scikit learn's @scikit-learn randomized search to optimize the hyperparameters for all three models. This process searched a predefined parameter grid to identify the configuration that maximized the F2 score.


![Confusion matrix of model performance on test data.](){#fig-conf-mtx width=60%}

# 4. Results and Discussion

**Final model selection**

After extensive preprocessing and EDA, we identified our best performing model among the three candidates - Decision Tree Classifier, Logistic Regression and Support Vector Machine (SVM) with Radial Basis Function (RBF) Kernel and Dummy Classifier as the baseline by using cross-validation and randomized search of hyperparameters. The SVM RBF achieved a validation score of **inline code 0.9646** - higher than the other two models. Our optimized model achieved a score of 0.98 when deployed on the test data - outperforming the validation score we achieved. 

There are two possible reasons which could have led to a higher test score than validation score on the unseen data. First, this could be due to the examples in the test set - the randomized split may have resulted in a test set similar to the trianing set. As a result it was easier for the model to predict better. Second, we maintained a 80-20 split on the small dataset, it is likely for the natural variance to contribute to the optimistic performace. 

# 5. Conclusion

Bringing our results together, SVM RBF with hyperparameters C = 1 and $ \gamma $ (Gamma) = 0.1 stands out as the best perfomer. Given that our objective is early detection of heart disease we have prioritized minimizing False Negatives - when a patient is likely to develop heart disease but have been identified healthy. The confusion matrix @fig-conf-mtx validates this performance, showing that the model successfully minimizes the False Negatives with **inline code Recall = 1.0 and Precision = 0.97**. Overall, we conclude that SVM with RBF kernel is well-suited for deployment in early stage detection of heart disease in patients.


# 6. References

